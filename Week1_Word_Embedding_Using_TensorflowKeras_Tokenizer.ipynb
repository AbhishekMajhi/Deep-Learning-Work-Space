{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "T9avvxsFkuK2"
      ],
      "authorship_tag": "ABX9TyOm/NKUYyFYutcPYwJEPolG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbhishekMajhi/Deep-Learning-Work-Space/blob/main/Week1_Word_Embedding_Using_TensorflowKeras_Tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tokenizer Introduction"
      ],
      "metadata": {
        "id": "T9avvxsFkuK2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67m1lnZKf-gw"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentenses = [\n",
        "    'I love my cat',\n",
        "    'I love animals'\n",
        "]"
      ],
      "metadata": {
        "id": "6Xgdq-ehgkVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=100) # will take top 100 words\n",
        "tokenizer.fit_on_texts(sentenses)\n",
        "word_index = tokenizer.word_index # returns a key a value pair where key is the word and value is the token for that word\n",
        "# tokenizer strips puntuation out"
      ],
      "metadata": {
        "id": "OMvbPXUdiO9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwvJTDShi0Do",
        "outputId": "5fe2399f-c579-4655-9c99-b6b146671e2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'i': 1, 'love': 2, 'my': 3, 'cat': 4, 'animals': 5}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text to sequence"
      ],
      "metadata": {
        "id": "nHnJqC2Ck1LW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "xIWLHElyi10G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\n",
        "    'I love my cat',\n",
        "    'I love animals',\n",
        "    'Do you also love animals?',\n",
        "    \"Don't you think my cat is cute?\"\n",
        "]"
      ],
      "metadata": {
        "id": "MhsMM-p-lAw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the tokenizer\n",
        "tokenizer = Tokenizer(num_words=100)\n",
        "# Tokenize the input sentences\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "# Get the word index dictionary\n",
        "word_index = tokenizer.word_index"
      ],
      "metadata": {
        "id": "SbWHJVOFlX-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate list of token sequences\n",
        "sequences = tokenizer.texts_to_sequences(sentences) # give us encoded list for each sentence"
      ],
      "metadata": {
        "id": "2NRAlOBxlnuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_index)\n",
        "print(sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52EGRQjHm2Z8",
        "outputId": "ed903fa3-a201-4cdf-af82-b0cfc45a0214"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'love': 1, 'i': 2, 'my': 3, 'cat': 4, 'animals': 5, 'you': 6, 'do': 7, 'also': 8, \"don't\": 9, 'think': 10, 'is': 11, 'cute': 12}\n",
            "[[2, 1, 3, 4], [2, 1, 5], [7, 6, 8, 1, 5], [9, 6, 10, 3, 4, 11, 12]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* If a word is not present in the dictionary at the time of training then that word will be not considered while generating inference.\n",
        "* So, we really need a big traing set so that our vocabulary will be big enough to cover most of the word while inference.\n",
        "* Instead of ignoring unseen words we can put some special value for these words, we can do that with tokenizer \"oov_token\"."
      ],
      "metadata": {
        "id": "96Pm5GBcnkrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=100, oov_token=\"<OOV>\")   # TOKEN should be something unique and distint\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "sequences = tokenizer.texts_to_sequences(sentences) # give us encoded list for each sentence\n",
        "print(word_index)\n",
        "print(sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lkgi0N3im_k0",
        "outputId": "b65918d5-1542-4296-da6c-0d7affc116ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'<OOV>': 1, 'love': 2, 'i': 3, 'my': 4, 'cat': 5, 'animals': 6, 'you': 7, 'do': 8, 'also': 9, \"don't\": 10, 'think': 11, 'is': 12, 'cute': 13}\n",
            "[[3, 2, 4, 5], [3, 2, 6], [8, 7, 9, 2, 6], [10, 7, 11, 4, 5, 12, 13]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Padding"
      ],
      "metadata": {
        "id": "09G9lof9sOur"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Before we feed data to train we need our sentences to be uniform in size. So, we need to do padding."
      ],
      "metadata": {
        "id": "yvfMKB7PsQpl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "fVkUCy0br9et"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\n",
        "    'I love my cat',\n",
        "    'I love animals',\n",
        "    'Do you also love animals?',\n",
        "    \"Don't you think my cat is cute?\"\n",
        "]"
      ],
      "metadata": {
        "id": "tsUGbJf_sxuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=100, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index"
      ],
      "metadata": {
        "id": "AUp3yxWmtD5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize the sentences\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "# add the padding token\n",
        "# by default the padding will be before the sentence so, use padding='post'\n",
        "# Max lenght a sequence can have is 100 as mentioned in num_words, but we can override that with 'maxlen'\n",
        "# Like padding maxlen will trim out from left of the sentence if it has more than 5 words but we set it to post with truncating='post'\n",
        "padded = pad_sequences(sequences, padding='post', truncating='post', maxlen=7)\n",
        "print(word_index)\n",
        "print()\n",
        "print(padded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugqZqrAxtjs9",
        "outputId": "da2f7054-5c4c-45f1-a540-c605a71fdad7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'<OOV>': 1, 'love': 2, 'i': 3, 'my': 4, 'cat': 5, 'animals': 6, 'you': 7, 'do': 8, 'also': 9, \"don't\": 10, 'think': 11, 'is': 12, 'cute': 13}\n",
            "\n",
            "[[ 3  2  4  5  0  0  0]\n",
            " [ 3  2  6  0  0  0  0]\n",
            " [ 8  7  9  2  6  0  0]\n",
            " [10  7 11  4  5 12 13]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VkAuYag2t6ZB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}